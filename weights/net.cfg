[net]
height=120
width=160
channels=3
downscale=4

[convolutional]
filters=8
size=3
stride=1
pad=2
dilation=2
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=16
size=3
stride=2
pad=1
dilation=1
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=32
size=3
stride=1
pad=2
dilation=2
activation=relu
hasBias=0

[convolutional]
filters=32
size=3
stride=2
pad=1
dilation=1
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=64
size=3
stride=1
pad=2
dilation=2
activation=relu
hasBias=0

[convolutional]
filters=64
size=3
stride=2
pad=1
dilation=1
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=128
size=3
stride=1
pad=2
dilation=2
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=128
size=3
stride=1
pad=2
dilation=2
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=128
size=3
stride=1
pad=2
dilation=2
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=128
size=3
stride=1
pad=2
dilation=2
activation=linear
hasBias=0

[batchnorm]
activation = relu

[convolutional]
filters=64
size=3
stride=1
pad=2
dilation=2
activation=linear
hasBias=0

[batchnorm]
activation = relu

[transposedconv]
filters=32
size=3
stride=2
pad=1
outpad=1
activation=linear

[batchnorm]
activation = relu

[shortcut]
activation=linear
from=6

[transposedconv]
filters=16
size=3
stride=2
pad=1
outpad=1
activation=linear

[batchnorm]
activation = relu

[shortcut]
activation=linear
from=3

[transposedconv]
filters=8
size=3
stride=2
pad=1
outpad=1
activation=linear

[batchnorm]
activation = relu

[shortcut]
activation=linear
from=1

[convolutional]
filters=5
size=1
stride=1
pad=0
activation=linear

[softmax]




